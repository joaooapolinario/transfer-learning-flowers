{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwyftBZXCBXPkR9TIFuq04"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bVFx00wsfe3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DEFINIR PARÂMETROS\n",
        "# ---------------------\n",
        "IMG_SIZE = (160, 160)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "gHia4iR1sp8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. CARREGAR DATASET DIRETAMENTE DA INTERNET (TFDS)\n",
        "# ----------------------------------------------------\n",
        "# O TFDS vai baixar e preparar o dataset 'tf_flowers' para nós.\n",
        "# Dividimos o conjunto de treino em 80% para treino e 20% para validação.\n",
        "(train_ds, validation_ds), ds_info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:]'], # Cria as divisões de treino e validação\n",
        "    with_info=True,\n",
        "    as_supervised=True, # Retorna os dados como tuplas (imagem, rótulo)\n",
        ")\n",
        "\n",
        "# Obter informações do dataset\n",
        "NUM_CLASSES = ds_info.features['label'].num_classes\n",
        "class_names = ds_info.features['label'].names\n",
        "num_train_examples = ds_info.splits['train[:80%]'].num_examples\n",
        "print(f\"Número de classes: {NUM_CLASSES}\")\n",
        "print(f\"Nomes das classes: {class_names}\")\n",
        "print(f\"Número de imagens de treino: {num_train_examples}\")\n"
      ],
      "metadata": {
        "id": "8BhahjXks9Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PRÉ-PROCESSAR OS DADOS\n",
        "# As imagens do dataset vêm em tamanhos variados. Precisamos redimensioná-las.\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    return image, label\n",
        "\n",
        "# Aplicar o redimensionamento e otimizar o carregamento\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_ds.map(format_image, num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "validation_dataset = validation_ds.map(format_image, num_parallel_calls=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "eMUdcONms-_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. CRIAR O MODELO BASE\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(160, 160, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False\n"
      ],
      "metadata": {
        "id": "aBN0YADVtBL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. ADICIONAR CAMADA DE CLASSIFICAÇÃO\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(NUM_CLASSES)(x) # Usamos a variável NUM_CLASSES\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "EmBny3rBtDqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. COMPILAR O MODELO\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nResumo do modelo:\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "8oLCcMURtGxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. TREINAR O MODELO\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "8D0Yy0q8tHbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Acurácia de Treinamento')\n",
        "plt.plot(val_acc, label='Acurácia de Validação')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Acurácia de Treinamento e Validação')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Perda de Treinamento')\n",
        "plt.plot(val_loss, label='Perda de Validação')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Perda de Treinamento e Validação')\n",
        "plt.xlabel('época')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1HzLH-FktI50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DESCONGELAR O MODELO BASE\n",
        "base_model.trainable = True\n",
        "\n",
        "# Vamos descongelar a partir da camada 100 (as primeiras continuam congeladas)\n",
        "fine_tune_at = 100\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nIniciando Fine-Tuning...\")\n",
        "\n",
        "# Continuar o treinamento por mais algumas épocas\n",
        "fine_tune_epochs = 2\n",
        "total_epochs =  EPOCHS + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history.epoch[-1],\n",
        "    validation_data=validation_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "EHJZXPICtOe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_url = \"https://blog.giulianaflores.com.br/wp-content/uploads/2013/06/Tulipas-coloridas.jpg\"\n",
        "image_name = image_url.split('/')[-1]\n",
        "\n",
        "# Baixa a imagem e salva localmente no Colab\n",
        "image_path = tf.keras.utils.get_file(image_name, origin=image_url)\n",
        "\n",
        "print(f\"Imagem de teste baixada em: {image_path}\")\n",
        "\n",
        "# --- CARREGAR E PRÉ-PROCESSAR A IMAGEM ---\n",
        "\n",
        "# Carrega a imagem do disco, redimensionando para o tamanho que o modelo espera (160x160)\n",
        "img = tf.keras.utils.load_img(image_path, target_size=IMG_SIZE)\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_batch = tf.expand_dims(img_array, 0)\n",
        "\n",
        "\n",
        "# --- FAZER A PREVISÃO ---\n",
        "\n",
        "# Usa o modelo treinado para prever a classe da imagem\n",
        "predictions = model.predict(img_batch)\n",
        "\n",
        "# O resultado 'predictions' são logits (números brutos).\n",
        "# Aplicamos a função Softmax para converter esses números em probabilidades (de 0 a 1)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "\n",
        "# --- EXIBIR O RESULTADO ---\n",
        "\n",
        "# Pega o nome da classe com a maior probabilidade\n",
        "predicted_class = class_names[np.argmax(score)]\n",
        "# Pega o valor da maior probabilidade\n",
        "confidence = 100 * np.max(score)\n",
        "\n",
        "# Imprime o resultado formatado\n",
        "print(f\"\\nO modelo classifica esta imagem como: '{predicted_class}'\")\n",
        "print(f\"Confiança: {confidence:.2f}%\")\n",
        "\n",
        "# Exibe a imagem testada com seu título de previsão\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Previsão: {predicted_class} ({confidence:.2f}%)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nGR7s3Yfy6hN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}